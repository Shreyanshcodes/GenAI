"""
We then provide a deep dive on the four main components.
## Document Loaders
How to load documents from a variety of sources.
## Text Splitters
An overview of the abstractions and implementions around splitting text.
## VectorStores
An overview of VectorStores and the many integrations LangChain provides.
## Retrievers
An overview of Retrievers and the implementations LangChain provides.
"""

# Document Loaders
## Using directory loader to load all .md files in a directory
from langchain.document_loaders import TextLoader
import os
import gradio as gr
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import DirectoryLoader
from langchain.document_loaders import PyPDFLoader
from langchain import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS

from langchain_community.llms import LlamaCpp
from langchain.document_loaders import DirectoryLoader
loader = DirectoryLoader('data/', glob="**/*.pdf", show_progress=True, loader_cls=PyPDFLoader)
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
docs = text_splitter.split_documents(documents)
print("PRINTING LEN OF DOCS----",len(docs))

# Embeddings

from langchain.embeddings import HuggingFaceEmbeddings  # create custom embeddings class that just calls API
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers-mini")

# Vector stores (pip install faiss or pip install faiss-cpu)
from langchain.vectorstores import FAISS
db = FAISS.from_documents(docs, embeddings)

# Retrievers
query = "How do I tow my car?"
retriever = db.as_retriever(search_kwargs={"k": 4})
filtered_docs = retriever.get_relevant_documents(query)
print(len(filtered_docs))

# LLM
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQAWithSourcesChain
question_prompt_template = """Use the following portion of a long document to see if any of the text is relevant to answer the question. 
Return any relevant text verbatim.
{context}
Question: {question}
Relevant text, if any:"""
QUESTION_PROMPT = PromptTemplate(
    template=question_prompt_template, input_variables=["context", "question"]
)

combine_prompt_template = """Given the following extracted parts of a long document and a question, create a final answer with references ("SOURCES"). 
If you don't know the answer, just say that you don't know. Don't try to make up an answer.
ALWAYS return a "SOURCES" part in your answer.
SOURCES:
QUESTION: {question}
=========
{summaries}
=========
FINAL ANSWER:"""
COMBINE_PROMPT = PromptTemplate(
    template=combine_prompt_template, input_variables=["summaries", "question"]
)

local_llm = "tinyllama-1.1b-chat-v1.0.Q2_K.gguf"
config = {
        'max_new_tokens':1024,
        'repetition_penalty':1.1,
        'temperature':0.1,
        'top_k':1,
        'top_p':0.9,
        'stream':True

    }
llm = LlamaCpp(
        model_path=local_llm,
        model_type="Llama",
        **config
    )
print("LLM Initialized....")
chain_type_kwargs = {"verbose": True, "combine_prompt": COMBINE_PROMPT, "question_prompt": QUESTION_PROMPT}
qa = RetrievalQAWithSourcesChain.from_chain_type(llm=llm, chain_type="map_reduce", 
                                retriever=retriever, return_source_documents=True,
                                chain_type_kwargs=chain_type_kwargs)
response = qa({"question": "How do I tow my car?", "verbose": True})
print(response.keys())
print(response["answer"])
print(response["sources"])
